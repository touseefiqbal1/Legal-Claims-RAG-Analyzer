https://legal-claims-rag-analyzer.streamlit.app/

---

## ğŸ“„ README â€” **Motor Claims Court Pack RAG Analyzer (MVP)**

---

# ğŸ§  Legal Claims RAG Analyzer

A **Retrieval-Augmented Generation (RAG)** system tailored for **legal and insurance claim documents**.
Upload synthetic or real case packs (multi-page PDFs) and ask natural-language questions â€” get **structured answers with citations** and **PDF page previews**.

This tool demonstrates:
âœ”ï¸ **Accurate extraction** of key claim fields
âœ”ï¸ **FAISS vector retrieval with LangChain**
âœ”ï¸ **Rule-based grounding + answer summarization**
âœ”ï¸ **Interactive Streamlit UI with PDF page viewer**
âœ”ï¸ **Evaluation metrics against ground truth**

---

## ğŸš€ Features

| Feature                                   | Status |
| ----------------------------------------- | ------ |
| Page-level PDF ingestion                  | âœ…      |
| FAISS vector index with semantic search   | âœ…      |
| Rule-based extraction + structured output | âœ…      |
| Pack selector (filter retrieval per case) | âœ…      |
| Clickable citations + PDF page viewer     | âœ…      |
| Evaluation dashboard (hit@k metrics)      | âœ…      |
| Exportable evaluation reports             | âœ…      |

---

## ğŸ“ Repository Structure

```
ğŸ“¦ Legal-Claims-RAG-Analyzer
 â”£ ğŸ“„ app.py                      # Streamlit UI
 â”£ ğŸ“„ evaluate.py                 # Evaluation script
 â”£ ğŸ“„ requirements.txt            # Python deps
 â”£ ğŸ“‚ data/sample_pdfs            # Place your PDF case packs here
 â”£ ğŸ“‚ indexes/faiss_index         # Persisted FAISS index + manifest
 â”£ ğŸ“‚ rag                        # RAG modules (ingestion, chunking, QA, extractors)
 â”ƒ â”£ ğŸ“„ ingest_pdf.py
 â”ƒ â”£ ğŸ“„ chunking.py
 â”ƒ â”£ ğŸ“„ index_faiss.py
 â”ƒ â”£ ğŸ“„ qa.py
 â”ƒ â”£ ğŸ“„ extractors.py
 â”— ğŸ“„ README.md
```

---

## ğŸ› ï¸ Getting Started

### ğŸ’¡ Requirements

* Python 3.9+
* Windows, Linux, macOS

---

## ğŸ§© Installation

```bash
git clone https://github.com/touseefiqbal1/Legal-Claims-RAG-Analyzer.git
cd Legal-Claims-RAG-Analyzer

# Create a virtual environment (recommended)
python -m venv venv
source venv/bin/activate   # (Windows) .\venv\Scripts\activate

pip install --upgrade pip
pip install -r requirements.txt
```

---

## ğŸ“„ Step 1 â€” Prepare Documents

Place your multi-page PDFs in:

```
data/sample_pdfs/
```

If using synthetic packs, they should include corresponding ground-truth JSON.

---

## ğŸ§  Step 2 â€” Build / Rebuild FAISS Index

Run the Streamlit app:

```bash
streamlit run app.py
```

In the sidebar â†’ **Build / Rebuild FAISS index**.

This does:

* Load PDFs page-by-page
* Chunk text
* Build vector index for semantic retrieval

---

## â“ Step 3 â€” Ask Questions

Use the UI:

* **Select a case pack** (or â€œAll packsâ€)
* Enter a question:

Examples:

```
What is the claim reference?
What is the total claimed amount?
List the reported injuries.
What fraud indicators were identified?
What is the suggested reserve?
```

The UI shows:

* A rule-based answer
* Extracted fields with supportive citations
* A clickable **view page** button with PDF preview

---

## ğŸ“Š Step 4 â€” Run Evaluation (with Manifest)

If you generated synthetic data with `manifest.json`, you can run evaluation:

1. Ensure path in sidebar â†’ **Manifest path**
   Example:

   ```
   indexes/manifest.json
   ```

2. Run evaluation from sidebar â†’ **Run evaluation**

3. Metrics shown:
   âœ” Hit rate (overall)
   âœ” Per-field hit rates
   âœ” Per-pack hit rates

Evaluation report is auto-saved as:

```
evaluation_report.json
```

---

## ğŸ“Œ Supported Queries (Preset Examples)

| Field             | Example Question                         |
| ----------------- | ---------------------------------------- |
| Claim reference   | `What is the claim reference?`           |
| Policy number     | `What is the policy number?`             |
| Incident details  | `When and where did the incident occur?` |
| Total claimed     | `What is the total claimed amount?`      |
| Reserve suggested | `What is the suggested reserve?`         |
| Injuries          | `List the reported injuries.`            |
| Fraud flags       | `What fraud indicators were identified?` |

---

## ğŸ§ª How It Works â€” Simplified

1. **Ingest PDF pages**

   * Metadata contains `source`, `path`, and `page`

2. **Chunk text for FAISS**

   * Splits into semantic chunks

3. **Vector store retrieval**

   * Uses semantic embeddings

4. **Rule-based extraction**

   * Extracts structured fields from retrieved text

5. **UI renders**

   * Answers, citations, PDF page previews

---

## ğŸ“Œ Notable Modules

| File                 | Responsibility                    |
| -------------------- | --------------------------------- |
| `rag/ingest_pdf.py`  | Load PDFs as page docs            |
| `rag/chunking.py`    | Chunk text w/ overlap             |
| `rag/index_faiss.py` | Build/persist FAISS               |
| `rag/qa.py`          | Retrieval + citations + grounding |
| `rag/extractors.py`  | Regex based field extraction      |
| `app.py`             | Streamlit UI                      |
| `evaluate.py`        | Evaluation script                 |

---

## ğŸ“ˆ Evaluation Explained

The evaluation computes **hit@k** for fields defined in your ground-truth JSON, such as:

* claim_reference
* policy_number
* incident_date/time
* incident_location
* police_reference
* total_claimed
* reserve_recommendation

It fetches **fetch_k** items, filters by pack, and computes whether the correct answer appears in the top-k results.

---

## ğŸ§  Tips for Better Results

âœ” Increase `Top-k` if some fields arenâ€™t found
âœ” Ensure PDFs are text-extractable (good OCR)
âœ” Build more diverse synthetic cases for evaluation
âœ” Tune regex patterns in `extractors.py` for real data

---
